# SemEval2024 Task8: Multidomain, Multimodel and Multilingual Machine-Generated Text Detection

### Description
Detecting machine-generated text is a critical task in the era of large language models. In this paper, we present our systems for SemEval-2024 Task 8, which focuses on multi-class classification to discern between human-written and maching-generated texts by five state-of-the-art large language models. We propose three different systems: unsupervised text similarity, triplet-loss-trained text similarity, and text classification. We show that the triplet-loss-trained text similarity system outperforms the other systems, achieving 80% accuracy on the test set and surpassing the baseline model for this subtask. Additionally, our text classification system, which takes into account sentence paraphrases generated by the candidate models, also outperforms the unsupervised text similarity system, achieving 74% accuracy.

### Contrastive Triplet Loss Trained Model

Text similarity models can also be trained on the provided training data.1 For this approach, we train a sentence transformer model with a triplet loss, which requires three inputs during training: anchor, positive, and negative samples (xi, xi+, xj −). This loss function aims to minimize the distance between the anchor and positive data (xi, xi+) while simultaneously maximizing the distance between the anchor and negative data(xi, xj −) (Ren and Xue, 2020). We conduct this training to enhance the vector representations of texts for multi-class classification.
